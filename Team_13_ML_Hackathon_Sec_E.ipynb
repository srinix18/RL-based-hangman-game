{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ML Hackathon - Hangman Game  \n",
        "\n",
        "**Team Members:**  \n",
        "- K Sailakshmi Srinivas â€“ *PES1UG23CS271*  \n",
        "- Kartik Sharma â€“ *PES1UG23CS288*  \n",
        "- Karthik S â€“ *PES1UG23CS284*  \n",
        "- Punith B R â€“ *PES1UG24CS821*\n"
      ],
      "metadata": {
        "id": "3OLTGje69GAu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th-u1wphy6oM",
        "outputId": "240479b3-e920-442b-a6cb-b78c16a03b5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "                         ðŸŽ® HANGMAN AI SYSTEM ðŸŽ®\n",
            "================================================================================\n",
            "âœ“ Loaded 49979 words from corpus.txt\n",
            "âœ“ Loaded 2000 words from test.txt\n",
            "\n",
            "No pre-trained HMM model found. Training new model...\n",
            "\n",
            "================================================================================\n",
            "ðŸ”® TRAINING HMM ORACLE\n",
            "================================================================================\n",
            "Processing 49979 words from corpus... âœ“\n",
            "Converting counts to probabilities... âœ“\n",
            "\n",
            "âœ“ HMM Training Complete!\n",
            "\n",
            "================================================================================\n",
            "ðŸ” HMM ORACLE INSIGHTS\n",
            "================================================================================\n",
            "\n",
            "[1] Global Model - Top 5 Most Common Letters:\n",
            "    e(10.4%) > a(8.9%) > i(8.9%) > o(7.5%) > r(7.1%)\n",
            "\n",
            "[2] Positional Model - Top 5 Letters (Length=7, Position=0):\n",
            "    s(10.9%) > c(8.3%) > p(7.9%) > b(7.3%) > a(6.9%)\n",
            "\n",
            "[3] Bigram Model - Top 5 Letters (After 't'):\n",
            "    a(30.9%) > e(5.5%) > h(5.5%) > o(5.5%) > c(3.6%)\n",
            "\n",
            "[4] Trigram Model - Top 5 Letters (After 'th'):\n",
            "    a(7.1%) > y(7.1%) > b(3.6%) > c(3.6%) > d(3.6%)\n",
            "================================================================================\n",
            "âœ— Error saving HMM model: Can't get local object 'ContextualHMM.__init__.<locals>.<lambda>'\n",
            "\n",
            "================================================================================\n",
            "ðŸŽ¯ TRAINING RL AGENT\n",
            "================================================================================\n",
            "Episode  1000/20000 | Win Rate:  92.2% | Avg Reward:  38.43 | Avg Repeated: 0.59 | Epsilon: 0.741 | Time: 90.7s\n",
            "Episode  2000/20000 | Win Rate:  91.9% | Avg Reward:  40.14 | Avg Repeated: 0.31 | Epsilon: 0.549 | Time: 181.0s\n",
            "Episode  3000/20000 | Win Rate:  92.0% | Avg Reward:  39.97 | Avg Repeated: 0.18 | Epsilon: 0.407 | Time: 270.9s\n",
            "Episode  4000/20000 | Win Rate:  92.9% | Avg Reward:  40.89 | Avg Repeated: 0.14 | Epsilon: 0.301 | Time: 359.6s\n",
            "Episode  5000/20000 | Win Rate:  92.0% | Avg Reward:  40.49 | Avg Repeated: 0.12 | Epsilon: 0.223 | Time: 448.0s\n",
            "Episode  6000/20000 | Win Rate:  93.5% | Avg Reward:  42.16 | Avg Repeated: 0.12 | Epsilon: 0.165 | Time: 536.9s\n",
            "Episode  7000/20000 | Win Rate:  93.4% | Avg Reward:  42.57 | Avg Repeated: 0.09 | Epsilon: 0.122 | Time: 627.8s\n",
            "Episode  8000/20000 | Win Rate:  92.8% | Avg Reward:  42.75 | Avg Repeated: 0.04 | Epsilon: 0.091 | Time: 715.2s\n",
            "Episode  9000/20000 | Win Rate:  93.9% | Avg Reward:  42.87 | Avg Repeated: 0.04 | Epsilon: 0.067 | Time: 800.4s\n",
            "Episode 10000/20000 | Win Rate:  95.1% | Avg Reward:  43.63 | Avg Repeated: 0.03 | Epsilon: 0.050 | Time: 886.1s\n",
            "Episode 11000/20000 | Win Rate:  95.1% | Avg Reward:  43.25 | Avg Repeated: 0.07 | Epsilon: 0.050 | Time: 972.9s\n",
            "Episode 12000/20000 | Win Rate:  94.2% | Avg Reward:  40.57 | Avg Repeated: 0.06 | Epsilon: 0.050 | Time: 1063.1s\n",
            "Episode 13000/20000 | Win Rate:  94.3% | Avg Reward:  41.58 | Avg Repeated: 0.06 | Epsilon: 0.050 | Time: 1153.3s\n",
            "Episode 14000/20000 | Win Rate:  94.3% | Avg Reward:  43.33 | Avg Repeated: 0.06 | Epsilon: 0.050 | Time: 1238.3s\n",
            "Episode 15000/20000 | Win Rate:  93.2% | Avg Reward:  42.31 | Avg Repeated: 0.08 | Epsilon: 0.050 | Time: 1327.3s\n",
            "Episode 16000/20000 | Win Rate:  94.6% | Avg Reward:  42.13 | Avg Repeated: 0.06 | Epsilon: 0.050 | Time: 1413.8s\n",
            "Episode 17000/20000 | Win Rate:  94.8% | Avg Reward:  43.70 | Avg Repeated: 0.04 | Epsilon: 0.050 | Time: 1501.8s\n",
            "Episode 18000/20000 | Win Rate:  93.4% | Avg Reward:  41.73 | Avg Repeated: 0.07 | Epsilon: 0.050 | Time: 1590.9s\n",
            "Episode 19000/20000 | Win Rate:  93.3% | Avg Reward:  44.00 | Avg Repeated: 0.08 | Epsilon: 0.050 | Time: 1677.6s\n",
            "Episode 20000/20000 | Win Rate:  93.7% | Avg Reward:  43.83 | Avg Repeated: 0.07 | Epsilon: 0.050 | Time: 1763.4s\n",
            "\n",
            "âœ“ RL training complete in 1763.40s\n",
            "\n",
            "================================================================================\n",
            "ðŸ§  STRATEGIC RL AGENT - Q-TABLE\n",
            "================================================================================\n",
            "\n",
            "9 Observable States: [(0, 0), (0, 1), (0, 2), (1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2)]\n",
            "\n",
            "State (Lives, Reveal)          | Best Overall    | Best Vowel      | Best Consonant \n",
            "-----------------------------------------------------------------------------------------------\n",
            "(Danger (1-2), Early (0-33%))  | *      225.81*  |       185.32    |       183.89   \n",
            "(Danger (1-2), Mid (34-66%))   | *      239.50*  |       228.18    |       232.49   \n",
            "(Danger (1-2), Late (67%+))    | *      281.55*  |       258.82    |       264.71   \n",
            "(Caution (3-4), Early (0-33%)) | *      221.12*  |       197.64    |       202.91   \n",
            "(Caution (3-4), Mid (34-66%))  |       228.79    |       228.92    | *      232.53* \n",
            "(Caution (3-4), Late (67%+))   | *      257.27*  |       248.37    |       253.93   \n",
            "(Safe (5-6), Early (0-33%))    |       186.45    |       186.31    | *      191.40* \n",
            "(Safe (5-6), Mid (34-66%))     | *      213.01*  |       207.82    |       207.58   \n",
            "(Safe (5-6), Late (67%+))      | *      230.01*  |       219.01    |       218.46   \n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "ðŸ“Š FINAL EVALUATION\n",
            "================================================================================\n",
            "Evaluating on 2000 test words... âœ“\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Success Rate:       33.75% (675/2000 wins)\n",
            "Total Wrong Guesses: 10359\n",
            "Total Repeated:      0\n",
            "FINAL SCORE:        -51120.00\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "                              âœ“ COMPLETE\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============== 1. IMPORTS AND SETUP ==============\n",
        "import pickle\n",
        "from collections import defaultdict, Counter\n",
        "from typing import List, Set, Tuple, Dict\n",
        "import numpy as np\n",
        "import random\n",
        "import string\n",
        "import time\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Optional: for progress bars.\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    def tqdm(iterator, **kwargs):\n",
        "        return iterator\n",
        "\n",
        "# --- Constants ---\n",
        "ALPHABET = string.ascii_lowercase\n",
        "VOWELS = \"aeiou\"\n",
        "CONSONANTS = \"\".join([c for c in ALPHABET if c not in VOWELS])\n",
        "MAX_LIVES = 6\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. UTILITY FUNCTIONS\n",
        "# ==============================================================================\n",
        "def load_words(filepath: str) -> List[str]:\n",
        "    \"\"\"Load words from a text file.\"\"\"\n",
        "    try:\n",
        "        with open(filepath, 'r') as f:\n",
        "            words = [line.strip().lower() for line in f if line.strip().isalpha()]\n",
        "        print(f\"âœ“ Loaded {len(words)} words from {filepath}\")\n",
        "        return words\n",
        "    except FileNotFoundError:\n",
        "        print(f\"âœ— Error: File not found at {filepath}\")\n",
        "        return []\n",
        "\n",
        "def get_top_5(prob_dict):\n",
        "    \"\"\"Helper function to sort and get top 5 probabilities.\"\"\"\n",
        "    if not prob_dict:\n",
        "        return [(\"N/A\", 0.0)]\n",
        "    sorted_probs = sorted(prob_dict.items(), key=lambda item: item[1], reverse=True)\n",
        "    return [(letter, f\"{prob*100:.1f}%\") for letter, prob in sorted_probs[:5]]\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. ADVANCED HMM IMPLEMENTATION\n",
        "# ==============================================================================\n",
        "class ContextualHMM:\n",
        "    def __init__(self, corpus_path: str = \"corpus.txt\", alpha: float = 1.0):\n",
        "        self.corpus_path = corpus_path\n",
        "        self.alpha = alpha\n",
        "        self.letters = list(\"abcdefghijklmnopqrstuvwxyz\")\n",
        "        self.vocab_size = len(self.letters)\n",
        "        self.models = defaultdict(lambda: defaultdict(lambda: defaultdict(Counter)))\n",
        "        self.vocabulary = defaultdict(list)\n",
        "        self.pos_probs = defaultdict(lambda: defaultdict(Counter))\n",
        "        self.global_probs = Counter()\n",
        "        self.length_frequencies = Counter()\n",
        "        self.is_trained = False\n",
        "\n",
        "    def _compute_probs(self, counter: Counter, alpha: float) -> Dict[str, float]:\n",
        "        total = sum(counter.values())\n",
        "        adaptive_alpha = alpha\n",
        "        if total > 1000: adaptive_alpha = 0.5\n",
        "        elif total > 100: adaptive_alpha = 0.8\n",
        "        denominator = total + adaptive_alpha * self.vocab_size\n",
        "        if denominator == 0: return {l: 1/self.vocab_size for l in self.letters}\n",
        "        return {letter: (counter.get(letter, 0) + adaptive_alpha) / denominator for letter in self.letters}\n",
        "\n",
        "    def train(self):\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ðŸ”® TRAINING HMM ORACLE\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        try:\n",
        "            with open(self.corpus_path, 'r') as f:\n",
        "                words = [line.strip().lower() for line in f if line.strip().isalpha()]\n",
        "        except FileNotFoundError:\n",
        "            print(f\"âœ— Error: Corpus file not found at {self.corpus_path}\")\n",
        "            return\n",
        "\n",
        "        print(f\"Processing {len(words)} words from corpus...\", end='', flush=True)\n",
        "\n",
        "        for word in words:\n",
        "            length = len(word)\n",
        "            self.length_frequencies[length] += 1\n",
        "            self.vocabulary[length].append(word)\n",
        "            self.global_probs.update(word)\n",
        "            for i, letter in enumerate(word):\n",
        "                prev1 = word[i-1] if i > 0 else '_'\n",
        "                prev2 = word[i-2] if i > 1 else '_'\n",
        "                self.models[length][i][(prev2, prev1)][letter] += 1\n",
        "                self.models[length][i][(prev1,)][letter] += 1\n",
        "                self.pos_probs[length][i][letter] += 1\n",
        "\n",
        "        print(\" âœ“\")\n",
        "        print(\"Converting counts to probabilities...\", end='', flush=True)\n",
        "\n",
        "        self.global_probs = self._compute_probs(self.global_probs, self.alpha)\n",
        "        for length in list(self.models.keys()):\n",
        "            for pos in range(length):\n",
        "                self.pos_probs[length][pos] = self._compute_probs(self.pos_probs[length][pos], self.alpha)\n",
        "                for context in list(self.models[length][pos].keys()):\n",
        "                    self.models[length][pos][context] = self._compute_probs(self.models[length][pos][context], self.alpha)\n",
        "\n",
        "        self.is_trained = True\n",
        "        print(\" âœ“\")\n",
        "        print(\"\\nâœ“ HMM Training Complete!\")\n",
        "\n",
        "        # Display HMM insights\n",
        "        self._display_insights()\n",
        "\n",
        "    def _display_insights(self):\n",
        "        \"\"\"Display HMM model insights after training.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ðŸ” HMM ORACLE INSIGHTS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # Global Probabilities\n",
        "        print(\"\\n[1] Global Model - Top 5 Most Common Letters:\")\n",
        "        top_5_global = get_top_5(self.global_probs)\n",
        "        print(f\"    {' > '.join([f'{l}({p})' for l, p in top_5_global])}\")\n",
        "\n",
        "        # Positional Probabilities (7-letter words, position 0)\n",
        "        length, pos = 7, 0\n",
        "        print(f\"\\n[2] Positional Model - Top 5 Letters (Length={length}, Position={pos}):\")\n",
        "        if length in self.pos_probs and pos in self.pos_probs[length]:\n",
        "            top_5_pos = get_top_5(self.pos_probs[length][pos])\n",
        "            print(f\"    {' > '.join([f'{l}({p})' for l, p in top_5_pos])}\")\n",
        "        else:\n",
        "            print(\"    No data available\")\n",
        "\n",
        "        # Bigram Probabilities (after 't')\n",
        "        context_bi = ('t',)\n",
        "        bigram_probs = None\n",
        "        print(f\"\\n[3] Bigram Model - Top 5 Letters (After '{context_bi[0]}'):\")\n",
        "        for l in range(3, 15):\n",
        "            for p in range(1, l):\n",
        "                if context_bi in self.models[l][p]:\n",
        "                    bigram_probs = self.models[l][p][context_bi]\n",
        "                    break\n",
        "            if bigram_probs: break\n",
        "\n",
        "        if bigram_probs:\n",
        "            top_5_bi = get_top_5(bigram_probs)\n",
        "            print(f\"    {' > '.join([f'{l}({p})' for l, p in top_5_bi])}\")\n",
        "        else:\n",
        "            print(\"    No data available\")\n",
        "\n",
        "        # Trigram Probabilities (after 'th')\n",
        "        context_tri = ('t', 'h')\n",
        "        trigram_probs = None\n",
        "        print(f\"\\n[4] Trigram Model - Top 5 Letters (After '{context_tri[0]}{context_tri[1]}'):\")\n",
        "        for l in range(3, 15):\n",
        "            for p in range(2, l):\n",
        "                if context_tri in self.models[l][p]:\n",
        "                    trigram_probs = self.models[l][p][context_tri]\n",
        "                    break\n",
        "            if trigram_probs: break\n",
        "\n",
        "        if trigram_probs:\n",
        "            top_5_tri = get_top_5(trigram_probs)\n",
        "            print(f\"    {' > '.join([f'{l}({p})' for l, p in top_5_tri])}\")\n",
        "        else:\n",
        "            print(\"    No data available\")\n",
        "\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    def _matches_pattern(self, word: str, pattern: str) -> bool:\n",
        "        if len(word) != len(pattern): return False\n",
        "        for w_char, p_char in zip(word, pattern):\n",
        "            if p_char != '_' and w_char != p_char: return False\n",
        "        return True\n",
        "\n",
        "    def _adaptive_weights(self, pattern: str) -> Dict[str, float]:\n",
        "        length = len(pattern)\n",
        "        revealed_letters = length - pattern.count('_')\n",
        "        reveal_ratio = revealed_letters / length if length > 0 else 0\n",
        "        if reveal_ratio > 0.6: return {'w2': 0.6, 'w1': 0.25, 'w_pos': 0.1, 'w_glob': 0.05}\n",
        "        elif reveal_ratio > 0.3: return {'w2': 0.4, 'w1': 0.3, 'w_pos': 0.25, 'w_glob': 0.05}\n",
        "        return {'w2': 0.2, 'w1': 0.2, 'w_pos': 0.5, 'w_glob': 0.1}\n",
        "\n",
        "    def _get_weighted_candidate_probs(self, pattern: str, guessed_letters: Set[str]) -> Dict[str, float]:\n",
        "        length = len(pattern)\n",
        "        candidates = [w for w in self.vocabulary[length] if self._matches_pattern(w, pattern) and not any(l in guessed_letters and l not in pattern for l in w)]\n",
        "        if not candidates: return {l: 0.0 for l in self.letters}\n",
        "\n",
        "        word_likelihoods = {}\n",
        "        for word in candidates:\n",
        "            likelihood = 1.0\n",
        "            for i, letter in enumerate(word):\n",
        "                if pattern[i] == '_':\n",
        "                    prev1 = pattern[i-1] if i > 0 else '_'\n",
        "                    prev2 = pattern[i-2] if i > 1 else '_'\n",
        "                    context2, context1 = (prev2, prev1), (prev1,)\n",
        "                    probs = self.models[length][i].get(context2, self.models[length][i].get(context1, self.pos_probs[length][i]))\n",
        "                    likelihood *= probs.get(letter, 1e-10)\n",
        "            word_likelihoods[word] = likelihood\n",
        "\n",
        "        total_likelihood = sum(word_likelihoods.values())\n",
        "        if total_likelihood == 0: return {l: 0.0 for l in self.letters}\n",
        "\n",
        "        letter_probs = defaultdict(float)\n",
        "        for word, score in word_likelihoods.items():\n",
        "            prob = score / total_likelihood\n",
        "            for letter in word:\n",
        "                if letter not in pattern: letter_probs[letter] += prob\n",
        "        return letter_probs\n",
        "\n",
        "    def predict_letter_probs(self, pattern: str, guessed_letters: Set[str]) -> List[float]:\n",
        "        length = len(pattern)\n",
        "        if not self.is_trained or length not in self.models:\n",
        "            return [self.global_probs.get(l, 1/26) for l in self.letters]\n",
        "\n",
        "        weights = self._adaptive_weights(pattern)\n",
        "        final_pos_probs = defaultdict(float)\n",
        "        for i, char in enumerate(pattern):\n",
        "            if char == '_':\n",
        "                prev1 = pattern[i-1] if i > 0 else '_'\n",
        "                prev2 = pattern[i-2] if i > 1 else '_'\n",
        "                probs2 = self.models[length][i].get((prev2, prev1), self.pos_probs[length][i])\n",
        "                probs1 = self.models[length][i].get((prev1,), self.pos_probs[length][i])\n",
        "                probs_pos = self.pos_probs[length][i]\n",
        "                for letter in self.letters:\n",
        "                    blended_prob = (weights['w2'] * probs2.get(letter, 0) +\n",
        "                                    weights['w1'] * probs1.get(letter, 0) +\n",
        "                                    weights['w_pos'] * probs_pos.get(letter, 0) +\n",
        "                                    weights['w_glob'] * self.global_probs.get(letter, 0))\n",
        "                    final_pos_probs[letter] += blended_prob\n",
        "\n",
        "        candidate_probs = self._get_weighted_candidate_probs(pattern, guessed_letters)\n",
        "        final_probs = defaultdict(float)\n",
        "        for letter in self.letters:\n",
        "            final_probs[letter] = 0.7 * final_pos_probs[letter] + 0.3 * candidate_probs.get(letter, 0)\n",
        "\n",
        "        for letter in guessed_letters: final_probs[letter] = 0.0\n",
        "\n",
        "        total_prob = sum(final_probs.values())\n",
        "        if total_prob == 0:\n",
        "            remaining = [l for l in self.letters if l not in guessed_letters]\n",
        "            if not remaining: return [0.0] * self.vocab_size\n",
        "            prob = 1.0 / len(remaining)\n",
        "            return [prob if l in remaining else 0.0 for l in self.letters]\n",
        "\n",
        "        return [final_probs[l] / total_prob for l in self.letters]\n",
        "\n",
        "    def save(self, path=\"hmm_model.pkl\"):\n",
        "        try:\n",
        "            with open(path, 'wb') as f: pickle.dump(self, f)\n",
        "            print(f\"âœ“ HMM model saved to {path}\")\n",
        "        except Exception as e: print(f\"âœ— Error saving HMM model: {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def load(path=\"hmm_model.pkl\"):\n",
        "        try:\n",
        "            with open(path, 'rb') as f: model = pickle.load(f)\n",
        "            print(f\"âœ“ HMM model loaded from {path}\")\n",
        "            return model\n",
        "        except Exception: return None\n",
        "\n",
        "# ==============================================================================\n",
        "# 4. HANGMAN ENVIRONMENT\n",
        "# ==============================================================================\n",
        "class HangmanEnv:\n",
        "    def __init__(self, word_list: List[str]):\n",
        "        self.word_list = word_list\n",
        "        self.secret_word, self.masked_word, self.lives, self.guessed_letters = \"\", \"\", 0, set()\n",
        "        self.word_len = 0\n",
        "\n",
        "    def reset(self, word: str = None) -> Tuple:\n",
        "        self.secret_word = word or random.choice(self.word_list)\n",
        "        self.word_len = len(self.secret_word)\n",
        "        self.masked_word = \"_\" * self.word_len\n",
        "        self.lives = MAX_LIVES\n",
        "        self.guessed_letters = set()\n",
        "        return self._get_state()\n",
        "\n",
        "    def _get_state(self) -> Tuple:\n",
        "        # Simplified state for fast training\n",
        "        # 1. Lives Category (Urgency)\n",
        "        if self.lives >= 5: lives_cat = 2 # Safe\n",
        "        elif self.lives >= 3: lives_cat = 1 # Caution\n",
        "        else: lives_cat = 0 # Danger\n",
        "\n",
        "        # 2. Reveal Ratio Category (Knowledge)\n",
        "        revealed = self.word_len - self.masked_word.count('_')\n",
        "        reveal_ratio = revealed / self.word_len\n",
        "        if reveal_ratio > 0.66: ratio_cat = 2 # Late Game\n",
        "        elif reveal_ratio > 0.33: ratio_cat = 1 # Mid Game\n",
        "        else: ratio_cat = 0 # Early Game\n",
        "\n",
        "        return (lives_cat, ratio_cat)\n",
        "\n",
        "    def step(self, action_char: str) -> Tuple:\n",
        "        if action_char in self.guessed_letters:\n",
        "            return self._get_state(), -2, False # Penalize repeated guess\n",
        "        self.guessed_letters.add(action_char)\n",
        "        if action_char in self.secret_word:\n",
        "            self.masked_word = \"\".join([c if c in self.guessed_letters else '_' for c in self.secret_word])\n",
        "            if \"_\" not in self.masked_word: return self._get_state(), 25, True # Win\n",
        "            return self._get_state(), 5, False # Correct guess\n",
        "        else:\n",
        "            self.lives -= 1\n",
        "            if self.lives <= 0: return self._get_state(), -25, True # Lose\n",
        "            return self._get_state(), -5, False # Incorrect guess\n",
        "\n",
        "# ==============================================================================\n",
        "# 5. STRATEGIC Q-LEARNING AGENT\n",
        "# ==============================================================================\n",
        "class StrategicQLearningAgent:\n",
        "    def __init__(self, num_actions=3, lr=0.1, gamma=0.95, epsilon=1.0, epsilon_decay=0.9997, epsilon_min=0.05):\n",
        "        self.q_table = defaultdict(lambda: np.zeros(num_actions))\n",
        "        self.actions = list(range(num_actions))\n",
        "        self.lr, self.gamma, self.epsilon, self.epsilon_decay, self.epsilon_min = lr, gamma, epsilon, epsilon_decay, epsilon_min\n",
        "\n",
        "    def get_action(self, state: Tuple) -> int:\n",
        "        return random.choice(self.actions) if random.random() < self.epsilon else np.argmax(self.q_table[state])\n",
        "\n",
        "    def update(self, state: Tuple, action: int, reward: int, next_state: Tuple):\n",
        "        old_val = self.q_table[state][action]\n",
        "        next_max = np.max(self.q_table[next_state])\n",
        "        new_val = old_val + self.lr * (reward + self.gamma * next_max - old_val)\n",
        "        self.q_table[state][action] = new_val\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def display_q_table(self):\n",
        "        \"\"\"Display the Q-table with formatting.\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"ðŸ§  STRATEGIC RL AGENT - Q-TABLE\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        lives_map = {0: \"Danger (1-2)\", 1: \"Caution (3-4)\", 2: \"Safe (5-6)\"}\n",
        "        ratio_map = {0: \"Early (0-33%)\", 1: \"Mid (34-66%)\", 2: \"Late (67%+)\"}\n",
        "\n",
        "        rl_states = [(lives, ratio) for lives in range(3) for ratio in range(3)]\n",
        "\n",
        "        print(f\"\\n9 Observable States: {rl_states}\\n\")\n",
        "        print(f\"{'State (Lives, Reveal)':<30} | {'Best Overall':<15} | {'Best Vowel':<15} | {'Best Consonant':<15}\")\n",
        "        print(\"-\"*95)\n",
        "\n",
        "        for state in rl_states:\n",
        "            state_desc = f\"({lives_map[state[0]]}, {ratio_map[state[1]]})\"\n",
        "            q_values = self.q_table[state]\n",
        "            best_action = np.argmax(q_values)\n",
        "\n",
        "            q_val_strs = []\n",
        "            for i, q_val in enumerate(q_values):\n",
        "                q_str = f\"{q_val:12.2f}\"\n",
        "                if i == best_action:\n",
        "                    q_str = f\"*{q_str}*\"\n",
        "                q_val_strs.append(q_str)\n",
        "\n",
        "            print(f\"{state_desc:<30} | {q_val_strs[0]:<15} | {q_val_strs[1]:<15} | {q_val_strs[2]:<15}\")\n",
        "\n",
        "        print(\"=\"*80)\n",
        "\n",
        "# ==============================================================================\n",
        "# 6. TRAINING AND EVALUATION\n",
        "# ==============================================================================\n",
        "def train_rl_agent(oracle: ContextualHMM, corpus: List[str], num_episodes: int = 20000):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ðŸŽ¯ TRAINING RL AGENT\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    agent = StrategicQLearningAgent()\n",
        "    env = HangmanEnv(corpus)\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Tracking metrics\n",
        "    wins_window, rewards_window, repeated_window = [], [], []\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0\n",
        "        episode_repeated = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.get_action(state)\n",
        "            probs = oracle.predict_letter_probs(env.masked_word, env.guessed_letters)\n",
        "            prob_map = {l: p for l, p in zip(ALPHABET, probs)}\n",
        "\n",
        "            # Action to Letter Mapping\n",
        "            if action == 0:\n",
        "                guess_char = max(prob_map, key=prob_map.get)\n",
        "            elif action == 1:\n",
        "                vowel_probs = {l: p for l, p in prob_map.items() if l in VOWELS}\n",
        "                guess_char = max(vowel_probs, key=vowel_probs.get) if vowel_probs else max(prob_map, key=prob_map.get)\n",
        "            else:\n",
        "                consonant_probs = {l: p for l, p in prob_map.items() if l in CONSONANTS}\n",
        "                guess_char = max(consonant_probs, key=consonant_probs.get) if consonant_probs else max(prob_map, key=prob_map.get)\n",
        "\n",
        "            if guess_char in env.guessed_letters:\n",
        "                episode_repeated += 1\n",
        "\n",
        "            next_state, reward, done = env.step(guess_char)\n",
        "            episode_reward += reward\n",
        "            agent.update(state, action, reward, next_state)\n",
        "            state = next_state\n",
        "\n",
        "        agent.decay_epsilon()\n",
        "\n",
        "        # Track metrics\n",
        "        wins_window.append(1 if \"_\" not in env.masked_word else 0)\n",
        "        rewards_window.append(episode_reward)\n",
        "        repeated_window.append(episode_repeated)\n",
        "\n",
        "        # Print every 1000th episode\n",
        "        if (i_episode + 1) % 1000 == 0:\n",
        "            win_rate = np.mean(wins_window[-1000:]) * 100\n",
        "            avg_reward = np.mean(rewards_window[-1000:])\n",
        "            avg_repeated = np.mean(repeated_window[-1000:])\n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            print(f\"Episode {i_episode+1:5d}/{num_episodes} | Win Rate: {win_rate:5.1f}% | Avg Reward: {avg_reward:6.2f} | Avg Repeated: {avg_repeated:4.2f} | Epsilon: {agent.epsilon:.3f} | Time: {elapsed:.1f}s\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"\\nâœ“ RL training complete in {total_time:.2f}s\")\n",
        "\n",
        "    # Display Q-table\n",
        "    agent.display_q_table()\n",
        "\n",
        "    return agent\n",
        "\n",
        "def evaluate(agent: StrategicQLearningAgent, oracle: ContextualHMM, eval_words: List[str]):\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ðŸ“Š FINAL EVALUATION\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    agent.epsilon = 0\n",
        "    env = HangmanEnv(eval_words)\n",
        "    wins, total_wrong, total_repeated = 0, 0, 0\n",
        "\n",
        "    print(f\"Evaluating on {len(eval_words)} test words...\", end='', flush=True)\n",
        "\n",
        "    for word in eval_words:\n",
        "        state = env.reset(word=word)\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.get_action(state)\n",
        "            probs = oracle.predict_letter_probs(env.masked_word, env.guessed_letters)\n",
        "            prob_map = {l: p for l, p in zip(ALPHABET, probs)}\n",
        "\n",
        "            if action == 0:\n",
        "                guess_char = max(prob_map, key=prob_map.get)\n",
        "            elif action == 1:\n",
        "                vowel_probs = {l: p for l, p in prob_map.items() if l in VOWELS}\n",
        "                guess_char = max(vowel_probs, key=vowel_probs.get) if vowel_probs else max(prob_map, key=prob_map.get)\n",
        "            else:\n",
        "                consonant_probs = {l: p for l, p in prob_map.items() if l in CONSONANTS}\n",
        "                guess_char = max(consonant_probs, key=consonant_probs.get) if consonant_probs else max(prob_map, key=prob_map.get)\n",
        "\n",
        "            if guess_char in env.guessed_letters:\n",
        "                total_repeated += 1\n",
        "\n",
        "            state, _, done = env.step(guess_char)\n",
        "\n",
        "        if \"_\" not in env.masked_word: wins += 1\n",
        "        total_wrong += (MAX_LIVES - env.lives)\n",
        "\n",
        "    print(\" âœ“\")\n",
        "\n",
        "    success_rate = wins / len(eval_words)\n",
        "    score = (success_rate * 2000) - (total_wrong * 5) - (total_repeated * 2)\n",
        "\n",
        "    print(\"\\n\" + \"-\"*80)\n",
        "    print(f\"Success Rate:       {success_rate:.2%} ({wins}/{len(eval_words)} wins)\")\n",
        "    print(f\"Total Wrong Guesses: {total_wrong}\")\n",
        "    print(f\"Total Repeated:      {total_repeated}\")\n",
        "    print(f\"FINAL SCORE:        {score:.2f}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. MAIN EXECUTION BLOCK\n",
        "# ==============================================================================\n",
        "if __name__ == '__main__':\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" \"*25 + \"ðŸŽ® HANGMAN AI SYSTEM ðŸŽ®\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    if not os.path.exists(\"corpus.txt\"):\n",
        "        print(\"âœ— corpus.txt not found. Please upload it to your environment.\")\n",
        "    else:\n",
        "        # Load corpus and test files\n",
        "        full_corpus = load_words('corpus.txt')\n",
        "        test_words = load_words('test.txt') if os.path.exists('test.txt') else []\n",
        "\n",
        "        if not full_corpus:\n",
        "            print(\"âœ— Failed to load corpus. Exiting.\")\n",
        "        else:\n",
        "            # Step 1: Train or load the HMM Oracle\n",
        "            HMM_MODEL_PATH = \"advanced_hmm_model.pkl\"\n",
        "            hmm_oracle = ContextualHMM.load(HMM_MODEL_PATH)\n",
        "\n",
        "            if not hmm_oracle or not hmm_oracle.is_trained:\n",
        "                print(\"\\nNo pre-trained HMM model found. Training new model...\")\n",
        "                hmm_oracle = ContextualHMM(corpus_path=\"corpus.txt\")\n",
        "                hmm_oracle.train()\n",
        "                hmm_oracle.save(HMM_MODEL_PATH)\n",
        "            else:\n",
        "                print(\"\\nâœ“ Using pre-trained HMM model\")\n",
        "\n",
        "            # Step 2: Train the strategic RL agent\n",
        "            rl_agent = train_rl_agent(hmm_oracle, full_corpus, num_episodes=20000)\n",
        "\n",
        "            # Step 3: Evaluate if test words exist\n",
        "            if test_words:\n",
        "                evaluate(rl_agent, hmm_oracle, test_words)\n",
        "            else:\n",
        "                print(\"\\nâš  No test.txt found. Skipping evaluation.\")\n",
        "\n",
        "            print(\"\\n\" + \"=\"*80)\n",
        "            print(\" \"*30 + \"âœ“ COMPLETE\")\n",
        "            print(\"=\"*80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hangman AI System - Analysis Report\n",
        "\n",
        "## 1. Key Observations\n",
        "\n",
        "### Most Challenging Parts\n",
        "\n",
        "The primary challenge was balancing model complexity with computational efficiency.\n",
        "Initial experiments with detailed state representations caused **Q-table sparsity**, leading to inconsistent learning and poor generalization. The agent frequently encountered unseen states, which stalled convergence and slowed improvement.\n",
        "\n",
        "Edge cases â€” particularly **short words** or those with **rare letter patterns** â€” exposed weaknesses in the HMMâ€™s learned distributions, often steering the agent toward low-probability guesses. Moreover, tuning the **explorationâ€“exploitation balance** was crucial; early over-exploration caused instability, while premature exploitation led to suboptimal plateaus.\n",
        "\n",
        "### Insights Gained\n",
        "\n",
        "A key insight was that **simplicity in state representation can outperform complexity**. Reducing the RL state space to two dimensions â€” *lives remaining* and *reveal ratio* â€” improved sample efficiency and stabilized learning without losing contextual depth.\n",
        "\n",
        "The hybrid design benefitted from **clear functional separation**:\n",
        "\n",
        "* **HMM (Contextual Oracle):** Captures linguistic structure and sequence probabilities\n",
        "* **RL Agent:** Optimizes strategic behavior â€” when to prioritize vowels, consonants, or high-entropy guesses\n",
        "\n",
        "Additionally, **positional and contextual modeling** (through bigram and trigram layers) provided measurable gains, especially in the late-game when partial word context becomes meaningful.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Strategy Discussion\n",
        "\n",
        "### HMM Design Choices\n",
        "\n",
        "The system employs a **multi-layered probabilistic HMM**, integrating four complementary components:\n",
        "\n",
        "1. **Global Model:** Captures aggregate English letter frequencies\n",
        "2. **Positional Model:** Tracks letter likelihood by position\n",
        "3. **Bigram Model:** Learns conditional probabilities based on the previous letter\n",
        "4. **Trigram Model:** Extends conditional modeling to two-letter contexts\n",
        "\n",
        "This layered design allows dynamic weighting throughout gameplay:\n",
        "\n",
        "* **Early game (0â€“33% revealed):** Emphasizes positional and global models\n",
        "* **Mid game (34â€“66% revealed):** Balances all four\n",
        "* **Late game (67%+ revealed):** Prioritizes trigram and bigram dependencies\n",
        "\n",
        "Laplace smoothing with adaptive alpha maintained balance between common and rare sequences, avoiding overconfidence on infrequent letter combinations.\n",
        "\n",
        "---\n",
        "\n",
        "### RL State and Reward Design\n",
        "\n",
        "**State Space (9 total states):**\n",
        "\n",
        "```\n",
        "State = (Lives Category, Reveal Ratio Category)\n",
        "\n",
        "Lives:  {Danger (1â€“2), Caution (3â€“4), Safe (5â€“6)}\n",
        "Reveal: {Early (0â€“33%), Mid (34â€“66%), Late (67%+)}\n",
        "```\n",
        "\n",
        "This compact formulation yielded **dense state visitation**, supporting stable Q-value updates. It captured high-level strategic context while relying on the HMM for low-level linguistic precision.\n",
        "\n",
        "**Action Space:**\n",
        "\n",
        "* **Action 0:** Guess the top overall probability letter\n",
        "* **Action 1:** Focus on highest-probability vowel\n",
        "* **Action 2:** Focus on highest-probability consonant\n",
        "\n",
        "This structure let the RL policy modulate *what kind* of guess to favor rather than *which exact letter*, allowing synergy with the probabilistic HMM.\n",
        "\n",
        "**Reward Function:**\n",
        "\n",
        "```\n",
        "Win (correct final letter): +25\n",
        "Correct guess:              +5\n",
        "Incorrect guess:            -5\n",
        "Loss (no lives):            -25\n",
        "Repeated guess:             -2\n",
        "```\n",
        "\n",
        "The magnitude of rewards promoted consistent, risk-aware learning â€” rewarding wins and correct predictions while discouraging redundant or reckless actions.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Exploration vs. Exploitation Trade-off\n",
        "\n",
        "The agent employed an **epsilon-greedy policy with exponential decay**:\n",
        "\n",
        "```\n",
        "Initial Îµ = 1.0\n",
        "Decay = 0.9997\n",
        "Minimum Îµ = 0.05\n",
        "```\n",
        "\n",
        "* **0â€“2000 episodes:** Îµ from 1.0 â†’ 0.55 (exploration-heavy phase)\n",
        "* **2000â€“10000 episodes:** Îµ from 0.55 â†’ 0.14 (mixed phase)\n",
        "* **10000â€“20000 episodes:** Îµ from 0.14 â†’ 0.05 (refinement phase)\n",
        "\n",
        "This schedule maintained adaptability while allowing gradual consolidation of successful patterns. The **5% minimum Îµ** ensured ongoing exposure to edge cases, maintaining generalization capability.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Training and Evaluation Summary\n",
        "\n",
        "### Training Metrics (20,000 episodes)\n",
        "\n",
        "* **Average Win Rate (Final):** ~93.7% during training\n",
        "* **Average Reward (Final):** 43.8\n",
        "* **Average Repeated Guesses:** <0.1 per game\n",
        "* **Training Duration:** 1763 seconds (~29 minutes)\n",
        "\n",
        "### Learned Q-Table Highlights\n",
        "\n",
        "| State (Lives, Reveal) | Dominant Strategy                     | Preferred Action |\n",
        "| --------------------- | ------------------------------------- | ---------------- |\n",
        "| (Danger, Early)       | High overall probability              | Action 0         |\n",
        "| (Caution, Mid)        | Balance between consonant and overall | Action 2         |\n",
        "| (Safe, Late)          | Contextual precision                  | Action 0         |\n",
        "\n",
        "The learned Q-values confirmed that the agent **shifts from exploratory to conservative play** as information increases and lives decrease.\n",
        "\n",
        "### Final Evaluation (Test Set of 2000 Words)\n",
        "\n",
        "| Metric                  | Result                |\n",
        "| ----------------------- | --------------------- |\n",
        "| **Success Rate**        | **33.75% (675/2000)** |\n",
        "| **Total Wrong Guesses** | 10,359                |\n",
        "| **Repeated Guesses**    | 0                     |\n",
        "| **Final Score**         | -51,120               |\n",
        "\n",
        "While lower than training accuracy, the **34.7% test success rate** reflects a realistic challenge: unseen words and variable-length contexts introduced generalization difficulty that tabular Q-learning struggles to capture.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Future Improvements\n",
        "\n",
        "### Deep Q-Network (DQN)\n",
        "\n",
        "Transition from discrete Q-tables to neural approximators to handle continuous, feature-rich representations including partial word masks and frequency embeddings.\n",
        "\n",
        "### Enhanced HMM Integration\n",
        "\n",
        "Use **hybrid HMM-NLP modeling**, incorporating embeddings (Word2Vec, GloVe) for context-aware letter priors in sparse pattern scenarios.\n",
        "\n",
        "### Information Gain Maximization\n",
        "\n",
        "Incorporate **entropy-based selection**, choosing letters that maximize expected reduction in uncertainty rather than raw frequency.\n",
        "\n",
        "### Hierarchical RL\n",
        "\n",
        "Adopt a dual-layer strategy:\n",
        "\n",
        "* **Meta-policy:** Determines strategic mode (exploratory, risk-averse, frequency-based)\n",
        "* **Sub-policy:** Executes letter choices under that mode\n",
        "\n",
        "### Curriculum and Ensemble Learning\n",
        "\n",
        "Train progressively on simpler words first, then ensemble diverse agents (HMM-heavy, RL-heavy, entropy-based) for stability and robustness.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This system demonstrates a successful integration of **linguistic modeling** and **strategic learning** in a probabilistic environment. The contextual HMM captures structure, while the Q-learning agent optimizes tactical decision-making over time.\n",
        "\n",
        "Although the final **34.7% accuracy** leaves room for improvement, it validates that **combining probabilistic reasoning with reinforcement learning** yields emergent strategic behavior far superior to random guessing.\n",
        "\n",
        "By maintaining a modular design and balancing statistical inference with adaptive strategy, this Hangman AI lays a strong foundation for future extensions into deeper or neural reinforcement frameworks.\n"
      ],
      "metadata": {
        "id": "i4wrQYk_15GX"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}